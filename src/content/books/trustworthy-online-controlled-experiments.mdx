---
aliases:
  - Trustworthy Online Controlled Experiments
  - trustworthy-online-controlled-experiments
author: 'Ron Kohavi, Diane Tang, and Ya Xu'
category: books
createdAt: 2025-12-07T13:06:38.000Z
lastHighlightedOn: 2025-12-05T04:56:00.000Z
publishedOn: null
source: kindle
status: seedling
tags: []
title: Trustworthy Online Controlled Experiments
updatedAt: '2025-12-05T04:56:00.000Z'
url: ''
contentType: books
---
## Highlights

- Thomke wrote that organizations will recognize maximal benefits from experimentation when it is used in conjunction with an “innovation system” (Thomke 2003). Agile software development is such an innovation system.
- Many organizations will not spend the resources required to define and measure progress. It is often easier to generate a plan, execute against it, and declare success, with the key metric being: “percent of plan delivered,” ignoring whether the feature has any positive impact to key metrics.
- The hard part is finding metrics measurable in a short period, sensitive enough to show differences, and that are predictive of long-term goals.
- most who have run controlled experiments in customer-facing websites and applications have experienced this humbling reality: we are poor at assessing the value of ideas.
- Interesting experiments are ones where the absolute difference between the expected outcome and the actual result is large.
- Experiments can help continuously iterate to better site redesigns, rather than having teams work on complete site redesigns that subject users to primacy effects (users are primed in the old feature, i.e., used to the way it works) and commonly fail not only to achieve their goals, but even fail to achieve parity with the old site on key metrics (Goward 2015, slides 22−24, Rawat 2018, Wolf 2018, Laja 2019).
- Our experience is that most big jumps fail (e.g., big site redesigns), yet there is a risk/reward tradeoff: the rare successes may lead to large rewards that compensate for many failures.
- We recommend that key metrics be normalized by the actual sample sizes, making revenue-per-user a good OEC.
- The latter, however, may not be as effective after the first couple of weeks as unique user growth is sub-linear due to repeat users while some metrics themselves have a “growing” variance over time
- Statistical power is the probability of detecting a meaningful difference between the variants when there really is one (statistically, reject the null when there is a difference).
- The user accumulation rate over time is also likely to be sub-linear given that the same user may return: if you have N users on day one, you will have fewer than 2N users after two days since some users visit on both days.
- It is important to ensure that your experiment captures the weekly cycle. We recommend running experiments for a minimum of one week.
- For example, selling gift cards may work well during the Christmas season but not as well during other times of the year. This is called external validity; the extent to which the results can be generalized, in this case to other periods of time.
- In general, overpowering an experiment is fine and even recommended, as sometimes we need to examine segments (e.g., geographic region or platform) and to ensure that the experiment has sufficient power to detect changes on several key metrics.
