---
aliases:
  - Latest | linus.zone
author: linus.zone
imageUrl: https://readwise-assets.s3.amazonaws.com/static/images/article0.00998d930354.png
permalink: l/articles/latest-linus-zone
publishedOn: None
source: web_clipper
status: 
title: Latest | linus.zone
url: https://linus.zone/latest
---
# Latest | linus.zone

![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article0.00998d930354.png)

## Metadata

- Author: [[linus.zone]]
- Full Title: Latest | linus.zone
- Category: #articles
- URL: https://linus.zone/latest

## Highlights

- evolution is a “compile-time” process, where favorable genetic influences are selected for at the beginning of life, intelligence is more of a “run-time” form of adaptation, where organisms can observe and respond to their environment closer to real-time.
    - Tags: [[intelligence]] [[learning]]
- In the short term, evolution rewards specific solutions that fit the environment, but in the long term, the trait that wins is one that lets its owners adapt quickly to any change in environment or rapidly conquer new environments.
    - Tags: [[intelligence]] [[learning]]
- Within DL systems like this, one way to understand model performance is as a tradeoff between optimizing for past examples and optimizing for future uncertainty. One training strategy might be to learn exactly as much as needed to be able to solve examples seem during training — kind of like rote memorization. This would be maximally efficient, but the model would be ill-prepared for the future uncertainty of new and novel examples. So better training methods made tradeoffs — the model learns extra information and abstractions that might not be necessary now, but might prepare it to deal with the uncertainty of novel future examples.
    - Tags: [[ai]] [[deep-learning]]
- Both of these ways of looking at intelligence learning abstractions, for future generalization tasks trading off learning efficiency, for future uncertainty are equivalent to another model of intelligence, as data compression. When a model learns how to recognize images from a dataset, it’s compressing the meaningful bits of information out of that dataset into a few gigabytes of data. It discards random noise that seems irrelevant, and “learns” (= “compresses”) the features and abstractions that seem important for potential future examples.
    - Tags: [[deep-learning]] [[intelligence]] [[learning]]
